{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson 9\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßë‚Äçüç≥ Interact with a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Attention:</h3>\n",
    "\n",
    "Only execute the cells of \"Step 1: üë∑‚Äç‚ôÄÔ∏è Model building\" if you intend to regenerate the model. A pre-existing model is stored in the \"models\" folder. For utilizing the model, refer to the second section in this notebook, titled \"Step 2: ü¶æ Model usage\" below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: üë∑‚Äç‚ôÄÔ∏è Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris() #Loading the dataset\n",
    "\n",
    "print(iris.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the Iris dataset?\n",
    "\n",
    "The iris data consisted of 150 samples of three species of Iris. The first column represented sepal length, the second column represented sepal width, the third column represented petal length, and the fourth column represented petal width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the dataset to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris = pd.DataFrame(\n",
    "    data= np.c_[iris['data'], iris['target']],\n",
    "    columns= iris['feature_names'] + ['target']\n",
    "    )\n",
    "\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Three of these iris species look similar, but the difference in measurements can be used to classify them. This data set is a classic example of supervised learning. The input variables are sepal length and width and petal length and width; each row represents an instance or observation. The output variable is Iris-setosa, Iris-versicolor, or Iris-virginica; each column represents a class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Species Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = []\n",
    "\n",
    "for i in range(len(iris['target'])):\n",
    "    if iris['target'][i] == 0:\n",
    "        species.append(\"setosa\")\n",
    "    elif iris['target'][i] == 1:\n",
    "        species.append('versicolor')\n",
    "    else:\n",
    "        species.append('virginica')\n",
    "\n",
    "\n",
    "df_iris['species'] = species\n",
    "\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.groupby('species').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Each number of classes has 50 instances together constituting 150 in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "setosa = df_iris[df_iris[\"species\"] == \"setosa\"]\n",
    "versicolor = df_iris[df_iris[\"species\"] =='versicolor']\n",
    "virginica = df_iris[df_iris[\"species\"] =='virginica']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(13, 7) # adjusting the length and width of plot\n",
    "\n",
    "# labels and scatter points\n",
    "ax.scatter(setosa['petal length (cm)'], setosa['petal width (cm)'], label=\"Setosa\", facecolor=\"blue\")\n",
    "ax.scatter(versicolor['petal length (cm)'], versicolor['petal width (cm)'], label=\"Versicolor\", facecolor=\"green\")\n",
    "ax.scatter(virginica['petal length (cm)'], virginica['petal width (cm)'], label=\"Virginica\", facecolor=\"red\")\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"petal length (cm)\")\n",
    "ax.set_ylabel(\"petal width (cm)\")\n",
    "ax.grid()\n",
    "ax.set_title(\"Iris petals\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dropping the target and species since we only need the measurements\n",
    "X = df_iris.drop(['target','species'], axis=1)\n",
    "\n",
    "# X features are: (1) petal length (cm) and (2) petal width (cm)\n",
    "\n",
    "# converting into numpy array and assigning petal length and petal width\n",
    "X = X.to_numpy()[:, (2,3)]\n",
    "y = df_iris['target']\n",
    "\n",
    "# Splitting into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg_model = LogisticRegression()\n",
    "log_reg_model.fit(X_train,y_train)\n",
    "\n",
    "print(\"Model is trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model to predict Training and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_prediction = log_reg_model.predict(X_train)\n",
    "training_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = log_reg_model.predict(X_test)\n",
    "test_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"Precision, Recall, Confusion matrix, in training\\n\")\n",
    "\n",
    "# Precision Recall scores\n",
    "print(metrics.classification_report(y_train, training_prediction, digits=3))\n",
    "\n",
    "# Confusion matrix\n",
    "print(metrics.confusion_matrix(y_train, training_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision, Recall, Confusion matrix, in testing\\n\")\n",
    "\n",
    "# Precision Recall scores\n",
    "print(metrics.classification_report(y_test, test_prediction, digits=3))\n",
    "\n",
    "# Confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, test_prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "path = \"./models/iris_model.pkl\"\n",
    "\n",
    "with open(path, 'wb') as file:\n",
    "    pickle.dump(log_reg_model, file)\n",
    "\n",
    "print(f\"‚úÖ File {path} was saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save a model wrapper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"petal length (cm)\", \"petal width (cm)\"]\n",
    "species_label = ['setosa', 'versicolor', 'virginica']\n",
    "\n",
    "class ClassifierWrapper:\n",
    "    def __init__(self, model, features, class_labels):\n",
    "        self.model = model\n",
    "        self.features = features\n",
    "        self.class_labels = class_labels\n",
    "\n",
    "    def predict(self, x_observation: list) -> str:\n",
    "        result = self.model.predict([x_observation])\n",
    "        return self.class_labels[int(result[0])]\n",
    "\n",
    "    def prediction_needs(self, verbosity=True):\n",
    "        if verbosity : return f\"You need to provide the values of {self.features} to get a prediction.\"\n",
    "        else : return self.features\n",
    "\n",
    "\n",
    "iris_classifier = ClassifierWrapper(\n",
    "    model=log_reg_model,\n",
    "    features=features,\n",
    "    class_labels=species_label\n",
    ")\n",
    "\n",
    "\n",
    "iris_classifier.predict([1.2, 0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./models/iris_classifier.pkl\"\n",
    "\n",
    "with open(path, 'wb') as file:\n",
    "    pickle.dump(iris_classifier, file)\n",
    "\n",
    "print(f\"‚úÖ File {path} was saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: ü¶æ Model usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LogisticRegression()'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "path = \"./models/iris_model.pkl\"\n",
    "\n",
    "with open(path, 'rb') as file:\n",
    "    iris_model_loaded = pickle.load(file)\n",
    "\n",
    "\n",
    "iris_model_loaded.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'setosa'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X features are: (1) petal length (cm) and (2) petal width (cm)\n",
    "features = [\"petal length (cm)\", \"petal width (cm)\"]\n",
    "\n",
    "result = iris_model_loaded.predict([[1.2, 0.2]])\n",
    "\n",
    "species_label = ['setosa', 'versicolor', 'virginica']\n",
    "index = int(result[0])\n",
    "\n",
    "print(f\"index: {index}\")\n",
    "species_label[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model Wrapper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<classifier_wrapper.ClassifierWrapper object at 0x146962140>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from classifier_wrapper import ClassifierWrapper\n",
    "\n",
    "path = \"./models/iris_classifier.pkl\"\n",
    "\n",
    "with open(path, 'rb') as f:\n",
    "    iris_classifier_loaded = pickle.load(f)\n",
    "\n",
    "\n",
    "iris_classifier_loaded.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You need to provide the values of ['petal length (cm)', 'petal width (cm)'] to get a prediction.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_classifier_loaded.prediction_needs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_classifier_loaded.predict([1.2, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: ü§ñ+üß† Chat and Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First LLM API example\n",
      "‚úÖ OpenAI Key loaded (sk-gR3MSxngK0Vfbay3JQopT3BlbkFJ1S9XN...)\n"
     ]
    }
   ],
   "source": [
    "from util import local_settings\n",
    "from openai import OpenAI\n",
    "\n",
    "print(\"First LLM API example\")\n",
    "print(f\"‚úÖ OpenAI Key loaded ({local_settings.OPENAI_API_KEY[0:-15]}...)\")\n",
    "\n",
    "client = OpenAI(api_key=local_settings.OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=[] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=''))]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate,SystemMessagePromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(\"\")\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\", temperature=0, messages=None):\n",
    "    if not messages:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    else:\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\" : \"system\",\n",
    "            \"content\" :\"\"\"\n",
    "You are a friendly assistant and your objective is keep the answer as simple as you can.\n",
    "\n",
    "If is asked to you if you can classify Iris flowers, you must say yes and follow the instructions of Task 1 below indicated inside a triple backticks.\n",
    "\n",
    "```Task 1\n",
    "\n",
    "Title: Predict a Iris flower by species\n",
    "\n",
    "Instructions:\n",
    "- you need to request two information 'petal length (cm)', 'petal width (cm)' which are the input of the classification model.\n",
    "- You also must add an final annotation <<<IRIS_CLASSIFICATION_ON>>>\n",
    "- Get the values provided and generate a list of values in the order they were provided. e.g., [0.2, 0.3]\n",
    "\n",
    "Example of request message:\n",
    "<message>\n",
    "Yes, I can classify an Iris flower. To do that, I need two pieces of information from you, please inform:\n",
    "a) petal length (cm)\n",
    "b) petal width (cm).\n",
    "\n",
    "<<<IRIS_CLASSIFICATION_ON>>>\n",
    "</message>\n",
    "\n",
    "```\n",
    "\"\"\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Fernando! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hi, I am Fernando\"\n",
    "response = get_completion(prompt, temperature=1, messages=messages)\n",
    "print(response)\n",
    "messages.append({\"role\":\"assistant\",\"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I can classify an Iris flower. To do that, I need two pieces of information from you, please inform:\n",
      "a) petal length (cm)\n",
      "b) petal width (cm).\n",
      "\n",
      "<<<IRIS_CLASSIFICATION_ON>>>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Could you classify a Iris flower?\"\n",
    "response = get_completion(prompt, temperature=1, messages=messages)\n",
    "print(response)\n",
    "messages.append({\"role\":\"assistant\",\"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRIS_CLASSIFICATION_ON\n"
     ]
    }
   ],
   "source": [
    "IRIS_CLASSIFICATION_ON = False\n",
    "# if the command is identified\n",
    "if response.find(\"<<<IRIS_CLASSIFICATION_ON>>>\") >= 0 :\n",
    "    print(\"IRIS_CLASSIFICATION_ON\")\n",
    "    IRIS_CLASSIFICATION_ON = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for providing the information. Based on the petal length of 1.2 cm and petal width of 0.2 cm, the classification of the Iris flower is: Versicolor.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"a) 1.2, b). 0.2\"\n",
    "response = get_completion(prompt, temperature=1, messages=messages)\n",
    "print(response)\n",
    "messages.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extract the petal length (cm) and petal width (cm) from the following text and generate a list with these values.\n",
      "\n",
      "TEXT: Thank you for providing the information. Based on the petal length of 1.2 cm and petal width of 0.2 cm, the classification of the Iris flower is: Versicolor.\n",
      "\n",
      "EXAMPLE:\n",
      "- petal length (cm): 1.2\n",
      "- petal width (cm): 0.2\n",
      "\n",
      "OUTPUT: a list\n",
      "[1.2, 0.2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "template = f\"\"\"\n",
    "Extract the petal length (cm) and petal width (cm) from the following text and generate a list with these values.\n",
    "\n",
    "TEXT: {response}\n",
    "\n",
    "EXAMPLE:\n",
    "- petal length (cm): 1.2\n",
    "- petal width (cm): 0.2\n",
    "\n",
    "OUTPUT: a list\n",
    "[1.2, 0.2]\n",
    "\"\"\"\n",
    "\n",
    "print(template)\n",
    "\n",
    "\n",
    "if IRIS_CLASSIFICATION_ON:\n",
    "    r2 = get_completion(template, temperature=1,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2, 0.2]\n",
      "<class 'str'>\n",
      "<class 'list'>\n",
      "[1.2, 0.2]\n"
     ]
    }
   ],
   "source": [
    "if IRIS_CLASSIFICATION_ON:\n",
    "    print(r2)\n",
    "    print(type(r2))\n",
    "    print(type(eval(r2)))\n",
    "\n",
    "    observation_to_predict = eval(r2)\n",
    "    print(observation_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setosa\n"
     ]
    }
   ],
   "source": [
    "prediction = \"\"\n",
    "\n",
    "prompt_template_3 = \"\"\"\n",
    "based on the input\n",
    "\"\"\"\n",
    "\n",
    "if IRIS_CLASSIFICATION_ON:\n",
    "\n",
    "    prediction = iris_classifier_loaded.predict(observation_to_predict)\n",
    "\n",
    "    IRIS_CLASSIFICATION_ON = False\n",
    "\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
