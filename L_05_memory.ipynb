{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson 5\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a786c77c",
   "metadata": {},
   "source": [
    "# üßë‚Äçüç≥ LangChain: Memory\n",
    "\n",
    "## Outline\n",
    "* Conversation<font color=\"cyan\">Buffer</font>Memory\n",
    "* Conversation<font color=\"cyan\">BufferWindow</font>Memory\n",
    "* Conversation<font color=\"cyan\">TokenBuffer</font>Memory\n",
    "* Conversation<font color=\"cyan\">Summary</font>Memory\n",
    "* Conversation<font color=\"cyan\">Entity</font>Memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301270b-7a69-40f8-9934-f840499b6ae9",
   "metadata": {},
   "source": [
    "> üî¥ <font color=\"red\">Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the class.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Conversational memory \n",
    "\n",
    "<h4>Large Language Models are <font color=\"yellow\">stateless</font> = each transaction is independent which means that they don't remember üò± </h4>\n",
    "\n",
    "> However, a conversational system should be able to access some window of past messages directly `MEMORY`. A memory system needs to support two basic actions: `reading` and `writing`.\n",
    "\n",
    "<img src=\"./img/lc_memory.png\" width=\"800px\"/>\n",
    "\n",
    "Chatbots simulate the presence of memory by incorporating `contextual` information from previous messages or additional details within the conversation. `LangChain` has outlined specific methods to maintain this context through a set of memory classes that replicate various memory behaviors and they are the following:\n",
    "\n",
    "* Conversation<font color=\"cyan\">Buffer</font>Memory\n",
    "* Conversation<font color=\"cyan\">BufferWindow</font>Memory\n",
    "* Conversation<font color=\"cyan\">TokenBuffer</font>Memory\n",
    "* Conversation<font color=\"cyan\">Summary</font>Memory\n",
    "* Conversation<font color=\"cyan\">KnowledgeGraph</font>Memory (Conversation<font color=\"cyan\">KG</font>Memory)\n",
    "* Conversation<font color=\"cyan\">Entity</font>Memory\n",
    "* Vector Store\n",
    "\n",
    "\n",
    "Conversational memory is what enables chatbots to respond to our queries in a conversational manner. It allows for coherent conversations by considering past interactions, rather than treating each query as independent. Without conversational memory, chatbots would lack the ability to remember and build upon previous interactions.\n",
    "\n",
    "üëâ Explore additional details about LangChain Memories at  https://python.langchain.com/docs/modules/memory/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f518f5",
   "metadata": {
    "height": 147,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First LLM API example\n",
      "‚úÖ OpenAI Key loaded (...QopT3BlbkFJ...)\n",
      "‚úÖ Model: gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "# Open AI\n",
    "from openai import OpenAI\n",
    "\n",
    "# LangChain\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Local\n",
    "from util import local_settings\n",
    "from env_colors import TerminalTextColor\n",
    "\n",
    "# Settings\n",
    "model=\"gpt-3.5-turbo\"\n",
    "\n",
    "print(\"First LLM API example\")\n",
    "print(f\"‚úÖ OpenAI Key loaded (...{local_settings.OPENAI_API_KEY[20:-20]}...)\")\n",
    "print(f\"‚úÖ Model: {model}\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297dcd5",
   "metadata": {},
   "source": [
    "## Conversation<b><font color=\"cyan\">Buffer</font></b>Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This memory allows for storing messages and then extracts the messages in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88bdf13d",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=model)\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db24677d",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Fernando! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Fernando\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc3ef937",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 is equal to 2.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acf3339a",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Fernando.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2529400d",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Fernando\n",
      "AI: Hello Fernando! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 is equal to 2.\n",
      "Human: What is my name?\n",
      "AI: Your name is Fernando.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5018cb0a",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Fernando\\nAI: Hello Fernando! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1+1 is equal to 2.\\nHuman: What is my name?\\nAI: Your name is Fernando.\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14219b70",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a36e9905",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61631b1f",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2fdf9ec",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ca79256",
   "metadata": {
    "height": 62,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "890a4497",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf98e9ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conversation<b><font color=\"cyan\">BufferWindow</font></b>Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversation<font color=\"cyan\">BufferWindow</font>Memory keeps a list of the interactions of the conversation over time. It only uses the last `K` interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66eeccc3",
   "metadata": {
    "height": 45,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ea6233e",
   "metadata": {
    "height": 45,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc4553fb",
   "metadata": {
    "height": 113,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a788403",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4087bc87",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=model)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4faaa952",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Fernando! How can I assist you today?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Fernando\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb20ddaa",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 is equal to 2.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "489b2194",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have access to personal information.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2931b92",
   "metadata": {},
   "source": [
    "## Conversation<b><font color=\"cyan\">TokenBuffer</font></b>Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversation<font color=\"cyan\">TokenBuffer</font>Memory keeps a buffer of recent interactions in memory, and uses `token length` rather than `number of interactions`(`k`) to determine when to flush interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb9020ed",
   "metadata": {
    "height": 79,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "llm = ChatOpenAI(temperature=0.0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43582ee6",
   "metadata": {
    "height": 147,
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "284288e1",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff55d5d",
   "metadata": {},
   "source": [
    "## Conversation<b><font color=\"cyan\">Summary</font></b>Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "72dcf8b1",
   "metadata": {
    "height": 45,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a5b238f",
   "metadata": {
    "height": 283,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"\"\"There is a meeting at 8am with your product team.\n",
    "You will need your PowerPoint presentation prepared.\n",
    "9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool.\n",
    "At Noon, lunch at the Italian restaurant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\"\"\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"},\n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e4ecabe",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human greets the AI and asks what is on the schedule for the day.\\nAI: There is a meeting at 8am with your product team.\\nYou will need your PowerPoint presentation prepared.\\n9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool.\\nAt Noon, lunch at the Italian restaurant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6728edba",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a221b1d",
   "metadata": {
    "height": 45,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human greets the AI and asks what is on the schedule for the day.\n",
      "AI: There is a meeting at 8am with your product team.\n",
      "You will need your PowerPoint presentation prepared.\n",
      "9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool.\n",
      "At Noon, lunch at the Italian restaurant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A good demo to show would be the Language Model API. It showcases the capabilities of our AI technology by generating coherent and contextually relevant text based on user input. It can be used for a variety of applications such as chatbots, content generation, and language translation.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb582617",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human greets the AI and asks what is on the schedule for the day. The AI informs the human that there is a meeting at 8am with the product team and advises them to have their PowerPoint presentation prepared. From 9am to 12pm, the human has time to work on their LangChain project, which will be efficient due to the power of LangChain. At noon, the human has a lunch appointment at an Italian restaurant with a customer who is traveling from over an hour away to learn about the latest advancements in AI. The AI reminds the human to bring their laptop to showcase the latest LLM demo.\\nHuman: What would be a good demo to show?\\nAI: A good demo to show would be the Language Model API. It showcases the capabilities of our AI technology by generating coherent and contextually relevant text based on user input. It can be used for a variety of applications such as chatbots, content generation, and language translation.'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Conversation<font color=\"cyan\"><b>Entity</b></font>Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationEntityMemory\n",
    "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    "from langchain.memory import ConversationEntityMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=model)\n",
    "\n",
    "memory = ConversationEntityMemory(llm=llm)\n",
    "_input = {\"input\": \"Deven & Sam are working on a hackathon project\"}\n",
    "memory.load_memory_variables(_input)\n",
    "memory.save_context(\n",
    "    _input,\n",
    "    {\"output\": \" That sounds like a great project! What kind of project are they working on?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Deven & Sam are working on a hackathon project\\nAI:  That sounds like a great project! What kind of project are they working on?',\n",
       " 'entities': {'Sam': 'Sam is currently working on a hackathon project with Deven.'}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": 'who is Sam'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=False,\n",
    "    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
    "\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'That sounds like a great project! What kind of project are they working on?'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Deven & Sam are working on a hackathon project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Deven': 'Deven is currently working on a hackathon project with Sam.',\n",
       " 'Sam': 'Sam is currently working on a hackathon project with Deven.'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory.entity_store.store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's interesting! Adding more complex memory structures to Langchain sounds like a challenging and exciting endeavor. Could you provide more details about what kind of memory structures they are working on?\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"They are trying to add more complex memory structures to Langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Deven & Sam are working on a hackathon project where they are trying to add more complex memory structures to Langchain. Adding these memory structures sounds like a challenging and exciting task. As for what I know about Deven & Sam, I don't have any specific information about them. Is there anything specific you would like to know about them?\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What do you know about Deven & Sam?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'That\\'s interesting! Adding a key-value store for entities mentioned so far in the conversation can be a useful feature. It would allow for efficient storage and retrieval of information related to those entities. This could be particularly helpful in a project like Langchain, where complex memory structures are being implemented.\\n\\nA key-value store is a type of database that stores data as a collection of key-value pairs. Each key is unique and is used to retrieve the corresponding value. In the context of Langchain, this key-value store could be used to associate entities mentioned in the conversation with their relevant information or attributes.\\n\\nFor example, if the conversation mentioned a specific entity like \"apple,\" the key-value store could store the key \"apple\" and its corresponding value could be a list of attributes such as color, taste, and nutritional information. This way, when the conversation refers to \"apple\" again, the key-value store can quickly retrieve the associated information.\\n\\nImplementing a key-value store for entities mentioned in the conversation can enhance the functionality and efficiency of Langchain. It can enable the system to better understand and remember the context of the conversation, making it more intelligent and capable of providing relevant responses.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"They are adding in a key-value store for entities mentioned so far in the conversation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Deven & Sam are the individuals working on the hackathon project. Unfortunately, I don't have any specific information about them beyond what you've mentioned. Is there anything specific you would like to know about them?\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What do you know about Deven & Sam?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Deven': 'Deven is currently working on a hackathon project with Sam.',\n",
      " 'Deven & Sam': 'Deven & Sam are the individuals working on the hackathon '\n",
      "                'project.',\n",
      " 'Langchain': 'Langchain is a project that Deven and Sam are working on for '\n",
      "              'the hackathon, where they are trying to add more complex memory '\n",
      "              'structures.',\n",
      " 'Sam': 'Sam is currently working on a hackathon project with Deven.'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(conversation.memory.entity_store.store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color=\"Yellow\">üëã the end </font></h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
