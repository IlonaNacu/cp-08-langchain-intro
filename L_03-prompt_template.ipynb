{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "073bf8f9",
   "metadata": {},
   "source": [
    "Lesson 3\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßë‚Äçüç≥ LangChain: Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline**\n",
    "\n",
    "* Direct API calls to OpenAI\n",
    "  \n",
    "* API calls through LangChain:\n",
    "  * Models  \n",
    "  * Prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üî¥ <font color=\"red\">Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the class.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01ff606",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[OpenAI API Key](https://platform.openai.com/account/api-keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First LLM API example\n",
      "‚úÖ OpenAI Key loaded (...QopT3BlbkFJ...)\n",
      "‚úÖ Model: gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from util import local_settings\n",
    "from openai import OpenAI\n",
    "\n",
    "from env_colors import TerminalTextColor\n",
    "\n",
    "model=\"gpt-3.5-turbo\"\n",
    "\n",
    "print(\"First LLM API example\")\n",
    "print(f\"‚úÖ OpenAI Key loaded (...{local_settings.OPENAI_API_KEY[20:-20]}...)\")\n",
    "print(f\"‚úÖ Model: {model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a92fb-8227-4513-8950-c965b822c425",
   "metadata": {},
   "source": [
    "> üîî <font color=\"#00d4d4\">**Note:** some characters of the key are omitted for security reasons.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=local_settings.OPENAI_API_KEY)\n",
    "\n",
    "def get_completion(prompt, temperature= 0, messages = [], model=model):\n",
    "\n",
    "    message = {\"role\": \"user\", \"content\": prompt}\n",
    "\n",
    "    messages.append(message)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"system\" : \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad9cdb",
   "metadata": {},
   "source": [
    "## Chat API : OpenAI\n",
    "\n",
    "Let's start with a direct API calls to OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "    Email --> Format\n",
    "    Style_1 --> Format\n",
    "    Prompt_template --> Format\n",
    "    Format --> Prompt\n",
    "    Prompt --> LLM\n",
    "    LLM --> Response\n",
    "```\n",
    "The chain view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer e-mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b32b57a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of\n",
    "cleaning up me kitchen. I need yer help right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c34459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "style = \"\"\"American English in a calm and respectful tone\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b558e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Translate the text that is delimited by triple backtick into a style that is {style}. In the end, add a portuguese translation of the response.\n",
    "\n",
    "text: ```{customer_email}```\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.findall(r'{(.*?)}', prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.format( style=style, customer_email=customer_email )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_inputs = { \"style\":style, \"customer_email\":customer_email }\n",
    "prompt.format( **prompt_inputs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Translate the text that is delimited by triple backtick into a style that is {style}.\n",
    "In the end, add a portuguese translation of the response.\n",
    "text: ```{customer_email}```\"\"\"\n",
    "\n",
    "print(f\"{TerminalTextColor.BLUE}----- PROMPT ------{TerminalTextColor.RESET}\")\n",
    "print(f\"{TerminalTextColor.GREEN}{prompt}{TerminalTextColor.RESET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c883dcbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = get_completion(prompt)\n",
    "print(f\"{TerminalTextColor.BLUE}----- RESPONSE ------{TerminalTextColor.RESET}\")\n",
    "print(f\"{TerminalTextColor.YELLOW}{response}{TerminalTextColor.RESET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80482d1",
   "metadata": {},
   "source": [
    "## Chat API : LangChain\n",
    "\n",
    "Let's try how we can do the same using LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "    Email --> ChatPromptTemplate_1\n",
    "    Style_1 --> ChatPromptTemplate_1\n",
    "    Prompt_template_1 --> ChatPromptTemplate_1\n",
    "    ChatPromptTemplate_1 --> Prompt_1\n",
    "    Prompt_1 --> LLM_1\"Chat\"\n",
    "    LLM_1\"Chat\" --> Result_1\n",
    "    Service_response --> ChatPromptTemplate_2\n",
    "    Result_1 --> ChatPromptTemplate_2\n",
    "    Style_2 --> ChatPromptTemplate_2\n",
    "    Prompt_template_2 --> ChatPromptTemplate_2\n",
    "    ChatPromptTemplate_2 --> Prompt_2\n",
    "    Prompt_2 --> LLM_2\n",
    "    LLM_2 --> Final_Response\n",
    "```\n",
    "\n",
    "The chain view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://mermaid.js.org/syntax/flowchart.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25c5b27",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d4a269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "model=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc0c8b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To control the randomness and creativity of the generated\n",
    "# text by an LLM, use temperature = 0.0\n",
    "chat = ChatOpenAI(temperature=0.0, model=model)\n",
    "\n",
    "print(f\"{TerminalTextColor.BLUE}----- chat ------{TerminalTextColor.RESET}\")\n",
    "print(chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d07256",
   "metadata": {},
   "source": [
    "### Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a31f246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bda7d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text that is delimited by triple backticks \\\n",
    "into a style that is {style}. In the end, add a portuguese translation of the response.\n",
    "\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "\n",
    "print(f\"{TerminalTextColor.BLUE}----- prompt_template ------{TerminalTextColor.RESET}\")\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac2cb16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"{TerminalTextColor.BLUE}----- ...messages[0].prompt ------{TerminalTextColor.RESET}\")\n",
    "\n",
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc5566c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"{TerminalTextColor.BLUE}----- input_variables ------{TerminalTextColor.RESET}\")\n",
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style and E-mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd51a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_style = \"\"\"American English in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_completion(prompt=f\"translate the following text to portuguese. TEXT: ```{customer_email}```\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff3954f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "    style=customer_style, text=customer_email\n",
    ")\n",
    "print(f\"{TerminalTextColor.BLUE}----- type of customer_messages ------{TerminalTextColor.RESET}\")\n",
    "print(type(customer_messages))\n",
    "\n",
    "print(f\"{TerminalTextColor.BLUE}----- type of customer_messages[0] ------{TerminalTextColor.RESET}\")\n",
    "print(type(customer_messages[0]))\n",
    "\n",
    "print(f\"{TerminalTextColor.BLUE}----- customer_messages[0] ------{TerminalTextColor.RESET}\")\n",
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd789f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat(customer_messages)\n",
    "print(f\"{TerminalTextColor.BLUE}----- RESPONSE ------{TerminalTextColor.RESET}\")\n",
    "\n",
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c267e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "service_reply = \"\"\"Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender.\n",
    "\n",
    "Tough luck! See ya!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff72bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "service_style_pirate = \"\"\"a polite tone that speaks in English Pirate\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e8f3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style_pirate,\n",
    "    text=service_reply)\n",
    "\n",
    "print(service_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae5552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "service_response = chat(service_messages)\n",
    "print(service_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color=\"Yellow\">üëã the end </font></h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
